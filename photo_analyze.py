# photo_analyze.py
# Generated by Copilot
#
"""
write python program to analyze and report on all photos under the directory O:\Bilder\1-d7100. Store the results in a Duckdb database called photos_db.db and include statistics on what kind of photos, e.g., landscape, birds, people, water, forest, spring, summer, autumn, winter along with camerra settings, date and time taken. Provide graphs showing type of photo, year taken, camera settings
"""
#!/usr/bin/env python3
"""
Photo analysis and reporting pipeline.
Scans directory, extracts EXIF, performs image tagging, stores results in DuckDB,
and creates summary graphs.
Author: Copilot
"""

import os
import sys
import io
from pathlib import Path
from datetime import datetime
from collections import Counter, defaultdict

import duckdb
import pandas as pd
from tqdm import tqdm
from PIL import Image, ExifTags, ImageStat, ImageOps
import piexif

import matplotlib.pyplot as plt
import seaborn as sns

# Try CLIP first (better multi-label descriptions), otherwise fallback to torchvision
USE_CLIP = True
try:
    import torch
    from torchvision import transforms, models
    # transformer-based CLIP via huggingface
    from transformers import CLIPProcessor, CLIPModel
except Exception:
    USE_CLIP = False

# If CLIP not available, we'll use a simple torchvision pretrained model
USE_TORCHVISION = True
try:
    import torch
    from torchvision import transforms, models
except Exception:
    USE_TORCHVISION = False

# Directory to scan and database path
ROOT_DIR = r"O:\Bilder\1-d7100"
DB_PATH = "photos_db.db"

# Supported image extensions
IMAGE_EXTS = {".jpg", ".jpeg", ".png", ".tiff", ".heic", ".heif", ".bmp", ".gif"}

# Target coarse categories and keywords to match against model outputs
COARSE_CATEGORIES = {
    "people": ["person", "man", "woman", "child", "face", "people", "selfie", "group"],
    "birds": ["bird", "crow", "sparrow", "eagle", "hen", "duck", "goose", "owl", "seagull", "hawk", "kite"],
    "water": ["sea", "ocean", "lake", "river", "water", "stream", "beach"],
    "forest": ["forest", "wood", "trees", "pine", "jungle"],
    "landscape": ["valley", "mountain", "cliff", "landscape", "field", "meadow", "horizon"],
    "snow": ["snow", "ski", "icy", "blizzard"],
    "spring": ["flower", "blossom", "bloom", "butterfly", "green"],
    "summer": ["sun", "beach", "swim", "sunset"],
    "autumn": ["autumn", "fall", "leaves", "leaf", "pumpkin"]
}

# Small helper: normalize text
def norm(s):
    return s.strip().lower()

# EXIF extraction
def extract_exif(path: Path):
    data = {
        "file_path": str(path),
        "file_name": path.name,
        "datetime_original": None,
        "camera_make": None,
        "camera_model": None,
        "exposure_time": None,
        "f_number": None,
        "iso": None,
        "focal_length": None,
        "width": None,
        "height": None
    }
    try:
        img = Image.open(path)
        data["width"], data["height"] = img.size
        # Use piexif when available for robust parsing
        try:
            ex = piexif.load(img.info.get("exif", b""))
            # 0th, ExifIFD
            zeroth = ex.get("0th", {})
            exif = ex.get("Exif", {})
            gps = ex.get("GPS", {})
            # helper to decode bytes
            def decode_if_bytes(v):
                if isinstance(v, bytes):
                    try:
                        return v.decode(errors="ignore")
                    except:
                        return str(v)
                return v
            if piexif.ImageIFD.Make in zeroth:
                data["camera_make"] = decode_if_bytes(zeroth.get(piexif.ImageIFD.Make))
            if piexif.ImageIFD.Model in zeroth:
                data["camera_model"] = decode_if_bytes(zeroth.get(piexif.ImageIFD.Model))
            if piexif.ExifIFD.DateTimeOriginal in exif:
                dto = decode_if_bytes(exif.get(piexif.ExifIFD.DateTimeOriginal))
                try:
                    data["datetime_original"] = datetime.strptime(dto, "%Y:%m:%d %H:%M:%S")
                except Exception:
                    data["datetime_original"] = dto
            if piexif.ExifIFD.ExposureTime in exif:
                data["exposure_time"] = exif.get(piexif.ExifIFD.ExposureTime)
            if piexif.ExifIFD.FNumber in exif:
                data["f_number"] = exif.get(piexif.ExifIFD.FNumber)
            if piexif.ExifIFD.ISOSpeedRatings in exif:
                iso = exif.get(piexif.ExifIFD.ISOSpeedRatings)
                data["iso"] = int(iso) if iso else None
            if piexif.ExifIFD.FocalLength in exif:
                data["focal_length"] = exif.get(piexif.ExifIFD.FocalLength)
        except Exception:
            # Fallback to PIL _getexif
            try:
                pil_exif = img._getexif()
                if pil_exif:
                    tag_map = {v: k for k, v in ExifTags.TAGS.items()}
                    # direct mappings
                    dto_tag = tag_map.get("DateTimeOriginal") or 36867
                    make_tag = tag_map.get("Make") or 271
                    model_tag = tag_map.get("Model") or 272
                    iso_tag = tag_map.get("ISOSpeedRatings") or 34855
                    exp_tag = tag_map.get("ExposureTime") or 33434
                    fn_tag = tag_map.get("FNumber") or 33437
                    fl_tag = tag_map.get("FocalLength") or 37386
                    if dto_tag in pil_exif:
                        try:
                            data["datetime_original"] = datetime.strptime(pil_exif[dto_tag], "%Y:%m:%d %H:%M:%S")
                        except:
                            data["datetime_original"] = pil_exif[dto_tag]
                    if make_tag in pil_exif:
                        data["camera_make"] = pil_exif[make_tag]
                    if model_tag in pil_exif:
                        data["camera_model"] = pil_exif[model_tag]
                    if iso_tag in pil_exif:
                        data["iso"] = pil_exif[iso_tag]
                    if exp_tag in pil_exif:
                        data["exposure_time"] = pil_exif[exp_tag]
                    if fn_tag in pil_exif:
                        data["f_number"] = pil_exif[fn_tag]
                    if fl_tag in pil_exif:
                        data["focal_length"] = pil_exif[fl_tag]
            except Exception:
                pass
    except Exception as e:
        print(f"Error reading EXIF for {path}: {e}", file=sys.stderr)
    return data

# Minimal color/season heuristic
def season_from_datetime(dt):
    if not isinstance(dt, datetime):
        return None
    m = dt.month
    if m in (12, 1, 2):
        return "winter"
    if m in (3, 4, 5):
        return "spring"
    if m in (6, 7, 8):
        return "summer"
    return "autumn"

def color_temperature_hint(img: Image.Image):
    # compute average green and saturation to guess "green" or "snow"
    img_small = ImageOps.exif_transpose(img).convert("RGB").resize((200,200))
    stat = ImageStat.Stat(img_small)
    r,g,b = stat.mean
    if (r+g+b) == 0:
        return None
    # very simple heuristics
    if g > r and g > b and g > 100:
        return "green"
    if r > 200 and g > 200 and b > 200:
        return "bright"
    if b > r and b > g and b > 120:
        return "blueish"
    return None

# Tagging machinery
class Tagger:
    def __init__(self, device=None):
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.use_clip = False
        self.use_torchvision = False
        # load CLIP if available and requested
        if USE_CLIP:
            try:
                self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(self.device)
                self.clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
                self.use_clip = True
            except Exception as e:
                print("CLIP model not available, falling back:", e)
                self.use_clip = False
        if not self.use_clip and USE_TORCHVISION:
            try:
                self.vgg = models.resnet50(pretrained=True).eval().to(self.device)
                from torchvision import transforms
                self.preprocess_torch = transforms.Compose([
                    transforms.Resize(256),
                    transforms.CenterCrop(224),
                    transforms.ToTensor(),
                    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                         std=[0.229, 0.224, 0.225]),
                ])
                # load ImageNet labels
                here = Path(__file__).parent
                # Try to load labels from torchvision
                import json, urllib.request
                # use bundled labels
                self.imagenet_labels = []
                try:
                    # torchvision has labels in a file online; fallback to built-in mapping
                    url = "https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json"
                    with urllib.request.urlopen(url, timeout=5) as f:
                        self.imagenet_labels = json.load(f)
                except Exception:
                    # built-in fallback minimal
                    self.imagenet_labels = [str(i) for i in range(1000)]
                self.use_torchvision = True
            except Exception as e:
                print("Torchvision model not available:", e)
                self.use_torchvision = False

    def predict_tags(self, image_path: Path, topk=10):
        tags = []
        try:
            img = Image.open(image_path).convert("RGB")
        except Exception as e:
            print(f"Cannot open {image_path}: {e}", file=sys.stderr)
            return tags

        if self.use_clip:
            try:
                inputs = self.clip_processor(text=["a photo of a " + k for k in sum([v for v in COARSE_CATEGORIES.values()], [])],
                                             images=img, return_tensors="pt", padding=True).to(self.device)
                with torch.no_grad():
                    outputs = self.clip_model(**inputs)
                    # CLIPModel here returns embeddings; we compute similarity manually
                    image_embeds = outputs.image_embeds  # shape (1, dim)
                    text_embeds = outputs.text_embeds  # shape (N, dim)
                    image_embeds = image_embeds / image_embeds.norm(dim=-1, keepdim=True)
                    text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)
                    sims = (100.0 * image_embeds @ text_embeds.T).softmax(dim=-1)
                    # pick top tokens with highest similarity
                    topk_idx = sims[0].argsort(descending=True)[:topk].cpu().numpy().tolist()
                    # map back to the corresponding texts
                    texts = ["a photo of a " + k for k in sum([v for v in COARSE_CATEGORIES.values()], [])]
                    tags = [texts[i].replace("a photo of a ", "") for i in topk_idx]
            except Exception as e:
                print("CLIP prediction failed, falling back to torchvision:", e, file=sys.stderr)
                tags = []
        if not tags and self.use_torchvision:
            try:
                x = self.preprocess_torch(img).unsqueeze(0).to(self.device)
                with torch.no_grad():
                    out = self.vgg(x)
                    probs = torch.nn.functional.softmax(out, dim=1)[0]
                    topk_idx = torch.topk(probs, topk).indices.cpu().numpy().tolist()
                    tags = [self.imagenet_labels[i] if i < len(self.imagenet_labels) else f"label{i}" for i in topk_idx]
            except Exception as e:
                print("torchvision predict failed:", e, file=sys.stderr)
                tags = []
        # final cleanup
        tags = [norm(str(t)) for t in tags if t]
        # also add simple color hint
        ct = color_temperature_hint(img)
        if ct:
            tags.append(ct)
        return list(dict.fromkeys(tags))  # preserve order, unique

# Map tags to coarse categories
def map_tags_to_categories(tags):
    cats = set()
    for cat, keywords in COARSE_CATEGORIES.items():
        for kw in keywords:
            for t in tags:
                if kw in t:
                    cats.add(cat)
    return list(cats)

# Create DuckDB schema and insert
def init_db(conn):
    conn.execute(f"""
    CREATE TABLE IF NOT EXISTS photos (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        file_path TEXT,
        file_name TEXT,
        width INTEGER,
        height INTEGER,
        datetime_original TIMESTAMP,
        year INTEGER,
        month INTEGER,
        season TEXT,
        camera_make TEXT,
        camera_model TEXT,
        exposure_time TEXT,
        f_number TEXT,
        iso INTEGER,
        focal_length TEXT,
        tags TEXT,
        categories TEXT
    );
    """)
    conn.commit()

def insert_record(conn, rec):
    conn.execute("""
    INSERT INTO photos (
        file_path, file_name, width, height, datetime_original, year, month, season,
        camera_make, camera_model, exposure_time, f_number, iso, focal_length, tags, categories
    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    """, (
        rec.get("file_path"),
        rec.get("file_name"),
        rec.get("width"),
        rec.get("height"),
        rec.get("datetime_original"),
        rec.get("year"),
        rec.get("month"),
        rec.get("season"),
        rec.get("camera_make"),
        rec.get("camera_model"),
        str(rec.get("exposure_time")) if rec.get("exposure_time") is not None else None,
        str(rec.get("f_number")) if rec.get("f_number") is not None else None,
        rec.get("iso"),
        str(rec.get("focal_length")) if rec.get("focal_length") is not None else None,
        ",".join(rec.get("tags") or []),
        ",".join(rec.get("categories") or [])
    ))
    conn.commit()

# Main scanning function
def scan_and_analyze(root_dir, db_path):
    p = Path(root_dir)
    if not p.exists():
        raise FileNotFoundError(f"{root_dir} not found")
    # prepare duckdb connection
    conn = duckdb.connect(db_path)
    init_db(conn)

    tagger = Tagger()
    # gather image files
    files = []
    for ext in IMAGE_EXTS:
        files.extend(p.rglob(f"*{ext}"))
    files = sorted(set(files))

    print(f"Found {len(files)} image files. Processing...")

    for fp in tqdm(files):
        exif = extract_exif(fp)
        tags = tagger.predict_tags(fp, topk=8)
        cats = map_tags_to_categories(tags)
        # seasonal tag
        season = season_from_datetime(exif.get("datetime_original"))
        if season:
            if season not in cats:
                cats.append(season)
        # derive year/month
        year = exif.get("datetime_original").year if isinstance(exif.get("datetime_original"), datetime) else None
        month = exif.get("datetime_original").month if isinstance(exif.get("datetime_original"), datetime) else None
        rec = dict(exif)
        rec.update({
            "tags": tags,
            "categories": cats,
            "season": season,
            "year": year,
            "month": month
        })
        insert_record(conn, rec)

    print("Processing complete. Generating reports.")
    generate_reports(conn)
    conn.close()
    print(f"All done. DB saved at {db_path} and graphs saved to current folder.")

# Reporting and plotting
def generate_reports(conn):
    df = conn.execute("SELECT * FROM photos").fetchdf()
    if df.empty:
        print("No photos to report.")
        return

    # explode categories into rows
    df["categories_list"] = df["categories"].fillna("").apply(lambda s: s.split(",") if s else [])
    df_expl = df.explode("categories_list")
    df_expl["categories_list"] = df_expl["categories_list"].replace("", pd.NA).dropna()

    sns.set(style="whitegrid")
    # 1) Photo type counts
    plt.figure(figsize=(10,6))
    type_counts = df_expl["categories_list"].value_counts().loc[lambda x:x.index.notna()]
    sns.barplot(x=type_counts.values, y=type_counts.index, palette="viridis")
    plt.title("Photo type counts")
    plt.xlabel("Count")
    plt.tight_layout()
    plt.savefig("photo_type_counts.png", dpi=150)
    plt.close()

    # 2) Photos per year
    plt.figure(figsize=(10,6))
    year_counts = df["year"].value_counts().sort_index()
    sns.barplot(x=year_counts.index.astype(str), y=year_counts.values, palette="magma")
    plt.title("Photos per year")
    plt.xlabel("Year")
    plt.ylabel("Count")
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig("photos_per_year.png", dpi=150)
    plt.close()

    # 3) Camera settings distributions: ISO, aperture, exposure_time (visualize top camera models too)
    # Clean ISO
    df_iso = df.dropna(subset=["iso"])
    if not df_iso.empty:
        plt.figure(figsize=(10,5))
        sns.histplot(df_iso["iso"].astype(int), bins=30, kde=False, color="steelblue")
        plt.title("ISO distribution")
        plt.xlabel("ISO")
        plt.tight_layout()
        plt.savefig("iso_distribution.png", dpi=150)
        plt.close()

    # Aperture (f_number may be rational in EXIF; try to parse common forms)
    def parse_rat(r):
        if pd.isna(r): return None
        try:
            s = str(r)
            if "/" in s:
                a,b = s.split("/")
                return float(a)/float(b)
            return float(s)
        except:
            return None

    df["f_number_parsed"] = df["f_number"].apply(parse_rat)
    df_fn = df.dropna(subset=["f_number_parsed"])
    if not df_fn.empty:
        plt.figure(figsize=(8,4))
        sns.countplot(x=df_fn["f_number_parsed"].round(1).astype(str), palette="coolwarm", order=sorted(df_fn["f_number_parsed"].unique()))
        plt.title("Aperture (F-number) distribution")
        plt.xlabel("F-number")
        plt.tight_layout()
        plt.savefig("aperture_distribution.png", dpi=150)
        plt.close()

    # Top camera models
    top_models = df["camera_model"].fillna("Unknown").value_counts().head(10)
    plt.figure(figsize=(10,5))
    sns.barplot(y=top_models.index, x=top_models.values, palette="cubehelix")
    plt.title("Top camera models")
    plt.xlabel("Count")
    plt.tight_layout()
    plt.savefig("top_camera_models.png", dpi=150)
    plt.close()

    # Save a summary CSV for quick inspection
    df.to_csv("photos_summary.csv", index=False)
    print("Graphs and summary CSV generated: photo_type_counts.png, photos_per_year.png, iso_distribution.png, aperture_distribution.png, top_camera_models.png, photos_summary.csv")

if __name__ == "__main__":
    if not Path(ROOT_DIR).exists():
        print(f"Root directory {ROOT_DIR} does not exist. Edit ROOT_DIR at top of script.")
        sys.exit(1)
    scan_and_analyze(ROOT_DIR, DB_PATH)
